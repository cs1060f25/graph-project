{
  "papers": [
    {
      "id": "1",
      "title": "Attention Is All You Need",
      "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar"],
      "year": 2017,
      "venue": "NIPS",
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
      "keywords": ["transformer", "attention", "neural networks", "sequence modeling"],
      "citations": ["2", "3", "4"],
      "citationCount": 25000
    },
    {
      "id": "2",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee"],
      "year": 2018,
      "venue": "NAACL",
      "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.",
      "keywords": ["bert", "transformer", "language understanding", "pre-training"],
      "citations": ["5", "6", "7"],
      "citationCount": 18000
    },
    {
      "id": "3",
      "title": "GPT-3: Language Models are Few-Shot Learners",
      "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder"],
      "year": 2020,
      "venue": "NeurIPS",
      "abstract": "We show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model.",
      "keywords": ["gpt", "language models", "few-shot learning", "scaling"],
      "citations": ["8", "9", "10"],
      "citationCount": 12000
    },
    {
      "id": "4",
      "title": "ResNet: Deep Residual Learning for Image Recognition",
      "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren"],
      "year": 2016,
      "venue": "CVPR",
      "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.",
      "keywords": ["resnet", "deep learning", "computer vision", "residual networks"],
      "citations": ["11", "12", "13"],
      "citationCount": 35000
    },
    {
      "id": "5",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal"],
      "year": 2019,
      "venue": "arXiv",
      "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results.",
      "keywords": ["roberta", "bert", "language models", "optimization"],
      "citations": ["14", "15"],
      "citationCount": 8000
    },
    {
      "id": "6",
      "title": "DistilBERT: A Distilled Version of BERT",
      "authors": ["Victor Sanh", "Lysandre Debut", "Julien Chaumond"],
      "year": 2019,
      "venue": "NeurIPS",
      "abstract": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under computational constraints and limited resources raises challenging questions.",
      "keywords": ["distilbert", "bert", "distillation", "efficiency"],
      "citations": ["16", "17"],
      "citationCount": 5000
    },
    {
      "id": "7",
      "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
      "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman"],
      "year": 2019,
      "venue": "ICLR",
      "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times.",
      "keywords": ["albert", "bert", "self-supervised learning", "efficiency"],
      "citations": ["18", "19"],
      "citationCount": 6000
    },
    {
      "id": "8",
      "title": "GPT-2: Language Models are Unsupervised Multitask Learners",
      "authors": ["Alec Radford", "Jeffrey Wu", "Rewon Child"],
      "year": 2019,
      "venue": "OpenAI",
      "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.",
      "keywords": ["gpt-2", "language models", "unsupervised learning", "multitask"],
      "citations": ["20", "21"],
      "citationCount": 9000
    },
    {
      "id": "9",
      "title": "T5: Text-to-Text Transfer Transformer",
      "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts"],
      "year": 2019,
      "venue": "JMLR",
      "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice.",
      "keywords": ["t5", "transformer", "transfer learning", "text-to-text"],
      "citations": ["22", "23"],
      "citationCount": 7000
    },
    {
      "id": "10",
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "authors": ["Aakanksha Chowdhery", "Sharan Narang", "Jacob Devlin"],
      "year": 2022,
      "venue": "arXiv",
      "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application.",
      "keywords": ["palm", "language models", "scaling", "pathways"],
      "citations": ["24", "25"],
      "citationCount": 3000
    },
    {
      "id": "11",
      "title": "VGGNet: Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "authors": ["Karen Simonyan", "Andrew Zisserman"],
      "year": 2014,
      "venue": "ICLR",
      "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters.",
      "keywords": ["vgg", "convolutional networks", "image recognition", "deep learning"],
      "citations": ["26", "27"],
      "citationCount": 28000
    },
    {
      "id": "12",
      "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
      "authors": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke"],
      "year": 2017,
      "venue": "AAAI",
      "abstract": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost.",
      "keywords": ["inception", "residual connections", "image recognition", "deep learning"],
      "citations": ["28", "29"],
      "citationCount": 15000
    },
    {
      "id": "13",
      "title": "DenseNet: Densely Connected Convolutional Networks",
      "authors": ["Gao Huang", "Zhuang Liu", "Laurens van der Maaten"],
      "year": 2017,
      "venue": "CVPR",
      "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet).",
      "keywords": ["densenet", "convolutional networks", "dense connections", "deep learning"],
      "citations": ["30", "31"],
      "citationCount": 12000
    }
  ]
}
